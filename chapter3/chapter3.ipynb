{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ardywibowo/cuda-mode/blob/main/chapter3/chapter3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b34981f1-9278-4ee8-a2de-57c6245041a7",
      "metadata": {
        "id": "b34981f1-9278-4ee8-a2de-57c6245041a7"
      },
      "source": [
        "# Chapter 3"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21fce84e-7b13-446e-9504-eb037dc26d03",
      "metadata": {
        "id": "21fce84e-7b13-446e-9504-eb037dc26d03"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "92cc0e45",
      "metadata": {
        "id": "92cc0e45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ff1879a-963a-464e-9ab0-399da6eb0348"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (1.11.1.1)\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Hit:6 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Fetched 119 kB in 2s (73.5 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "32 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "g++-11 is already the newest version (11.4.0-1ubuntu1~22.04).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 32 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ccache is already the newest version (4.5.1-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 32 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!pip install ninja\n",
        "!sudo apt update\n",
        "!sudo apt install g++-11 -y\n",
        "!sudo apt install ccache -y"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.utils.cpp_extension\n",
        "import os\n",
        "os.environ['CXX'] = '/usr/lib/ccache/g++-11'\n",
        "os.environ['CC'] = '/usr/lib/ccache/gcc-11'"
      ],
      "metadata": {
        "id": "eWzrYElusXY6"
      },
      "id": "eWzrYElusXY6",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cuda_begin = \"\"\"\n",
        "//cuda\n",
        "#include <torch/extension.h>\n",
        "#include <stdio.h>\n",
        "#include <c10/cuda/CUDAException.h>\n",
        "\n",
        "#define CHECK_CUDA(x) TORCH_CHECK(x.device().is_cuda(), #x \" must be a CUDA tensor\")\n",
        "#define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x \" must be contiguous\")\n",
        "#define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
        "\n",
        "inline unsigned int cdiv(unsigned int a, unsigned int b) { return (a + b - 1) / b;}\n",
        "//!cuda\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "OaGV394NPYMl"
      },
      "id": "OaGV394NPYMl",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 1\n",
        "\n",
        "In this chapter we implemented a matrix multiplication kernel that has each thread produce one output matrix element. In this question, you will implement different matrix-matrix multiplication kernels and compare them.\n",
        "\n",
        "a. Write a kernel that has each thread produce one output matrix row. Fill in the execution configuration parameters for the design."
      ],
      "metadata": {
        "id": "pW6QL__iY8Hm"
      },
      "id": "pW6QL__iY8Hm"
    },
    {
      "cell_type": "code",
      "source": [
        "cuda_src = cuda_begin + \\\n",
        "\"\"\"\n",
        "//cuda\n",
        "__global__ void matmul_row(float* m, float* n, float* out, int h, int w, int k) {\n",
        "    int r = blockIdx.x*blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (r >= h) return;\n",
        "\n",
        "    for (int c = 0; c < w; ++c) {\n",
        "        float o = 0;\n",
        "        for (int i = 0; i<k; ++i) {\n",
        "            o += m[r*k + i] * n[i*w + c];\n",
        "        }\n",
        "        out[r*w+c] = o;\n",
        "    }\n",
        "}\n",
        "\n",
        "torch::Tensor matmul(torch::Tensor m, torch::Tensor n) {\n",
        "    CHECK_INPUT(m); CHECK_INPUT(n);\n",
        "    int h = m.size(0);\n",
        "    int w = n.size(1);\n",
        "    int k = m.size(1);\n",
        "    TORCH_CHECK(k == n.size(0), \"Size mismatch!\");\n",
        "    auto output = torch::zeros({h, w}, m.options());\n",
        "\n",
        "    dim3 tpb(256);\n",
        "    dim3 blocks(cdiv(h, tpb.x));\n",
        "    matmul_row<<<blocks, tpb>>>(\n",
        "        m.data_ptr<float>(), n.data_ptr<float>(), output.data_ptr<float>(), h, w, k);\n",
        "    C10_CUDA_KERNEL_LAUNCH_CHECK();\n",
        "    return output;\n",
        "}\n",
        "//!cuda\n",
        "\"\"\"\n",
        "\n",
        "cpp_src = \\\n",
        "\"\"\"\n",
        "//cuda\n",
        "torch::Tensor matmul(torch::Tensor m, torch::Tensor n);\n",
        "//!cuda\n",
        "\"\"\"\n",
        "\n",
        "module = torch.utils.cpp_extension.load_inline(\n",
        "    \"test_ext\", cpp_src, cuda_src,\n",
        "    functions=['matmul'], extra_cuda_cflags=['--ptxas-options=-v'], verbose=True)\n",
        "\n",
        "n = 32\n",
        "A = torch.randn(n, n, device='cuda')\n",
        "B = torch.randn(n, n, device='cuda')\n",
        "\n",
        "A = torch.ones((3, 3), device='cuda')\n",
        "B = torch.ones((3, 3), device='cuda')\n",
        "\n",
        "out = module.matmul(A, B); torch.cuda.synchronize()\n",
        "reference = torch.matmul(A, B)\n",
        "print(\"Out:\", out)\n",
        "print(\"Reference:\", reference)\n",
        "print(\"Correct Implementation:\", torch.allclose(out, reference))\n",
        "\n",
        "import time\n",
        "num_trials = 1_000\n",
        "\n",
        "with torch.profiler.profile() as prof:\n",
        "    for i in range(num_trials):\n",
        "        module.matmul(A, B)\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "print(prof.key_averages().table())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2RQMCWatB7r",
        "outputId": "c021aea7-7d81-45bc-b001-ce9894bf0524"
      },
      "id": "w2RQMCWatB7r",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
            "No modifications detected for re-loaded extension module test_ext_v2, skipping build step...\n",
            "Loading extension module test_ext_v2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Out: tensor([[ -2.4887,  -1.7471,   2.4387,  ...,   0.1727,   1.1549,   1.4592],\n",
            "        [ -3.4692,   8.6338,  -2.4712,  ...,   3.5144,   0.9575,   2.7203],\n",
            "        [ -6.9637,   5.5027,   1.5716,  ...,   3.4485,   0.2339,   6.3301],\n",
            "        ...,\n",
            "        [  1.2553,  -0.8796,  -5.3700,  ...,   1.4417,   1.7112,   3.1901],\n",
            "        [ 11.2950,   1.3072,  -3.6682,  ...,  10.2170,  -3.2171,  -1.7322],\n",
            "        [-10.9152,  -0.5431,  -4.4932,  ...,  -6.4946,  -2.2655,  -9.4765]],\n",
            "       device='cuda:0')\n",
            "Reference: tensor([[ -2.4887,  -1.7471,   2.4387,  ...,   0.1727,   1.1549,   1.4592],\n",
            "        [ -3.4692,   8.6338,  -2.4712,  ...,   3.5144,   0.9575,   2.7203],\n",
            "        [ -6.9637,   5.5027,   1.5716,  ...,   3.4485,   0.2339,   6.3301],\n",
            "        ...,\n",
            "        [  1.2553,  -0.8796,  -5.3700,  ...,   1.4417,   1.7112,   3.1901],\n",
            "        [ 11.2950,   1.3072,  -3.6682,  ...,  10.2170,  -3.2171,  -1.7322],\n",
            "        [-10.9152,  -0.5431,  -4.4932,  ...,  -6.4946,  -2.2655,  -9.4765]],\n",
            "       device='cuda:0')\n",
            "Correct Implementation: True\n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                            aten::zeros         4.08%       4.748ms        38.41%      44.696ms      44.696us       0.000us         0.00%       1.954ms       1.954us          1000  \n",
            "                                            aten::empty        10.03%      11.671ms        10.03%      11.671ms      11.671us       0.000us         0.00%       0.000us       0.000us          1000  \n",
            "                                            aten::zero_         3.93%       4.579ms        24.77%      28.827ms      28.827us       0.000us         0.00%       2.000ms       2.000us          1000  \n",
            "                                            aten::fill_         8.78%      10.215ms        20.84%      24.248ms      24.248us       2.000ms         2.38%       2.000ms       2.000us          1000  \n",
            "                                       cudaLaunchKernel        17.99%      20.936ms        17.99%      20.936ms      10.468us       0.000us         0.00%       0.000us       0.000us          2000  \n",
            "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       2.000ms         2.38%       2.000ms       2.000us          1000  \n",
            "      matmul_row(float*, float*, float*, int, int, int)         0.00%       0.000us         0.00%       0.000us       0.000us      82.000ms        97.62%      82.000ms      82.000us          1000  \n",
            "                                  cudaDeviceSynchronize        55.19%      64.219ms        55.19%      64.219ms      64.155us       0.000us         0.00%       0.000us       0.000us          1001  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 116.368ms\n",
            "Self CUDA time total: 84.000ms\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "b. Write a kernel that has each thread produce one output matrix column. Fill in the execution configuration parameters for the design."
      ],
      "metadata": {
        "id": "oCjfrcr1hzdT"
      },
      "id": "oCjfrcr1hzdT"
    },
    {
      "cell_type": "code",
      "source": [
        "cuda_src = cuda_begin + \\\n",
        "\"\"\"\n",
        "//cuda\n",
        "__global__ void matmul_col(float* m, float* n, float* out, int h, int w, int k) {\n",
        "    int c = blockIdx.x*blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (c >= w) return;\n",
        "\n",
        "    for (int r = 0; r < h; ++r) {\n",
        "        float o = 0;\n",
        "        for (int i = 0; i<k; ++i) {\n",
        "            o += m[r*k + i] * n[i*w + c];\n",
        "        }\n",
        "        out[r*w+c] = o;\n",
        "    }\n",
        "}\n",
        "\n",
        "torch::Tensor matmul(torch::Tensor m, torch::Tensor n) {\n",
        "    CHECK_INPUT(m); CHECK_INPUT(n);\n",
        "    int h = m.size(0);\n",
        "    int w = n.size(1);\n",
        "    int k = m.size(1);\n",
        "    TORCH_CHECK(k == n.size(0), \"Size mismatch!\");\n",
        "    auto output = torch::zeros({h, w}, m.options());\n",
        "\n",
        "    dim3 tpb(256);\n",
        "    dim3 blocks(cdiv(h, tpb.x));\n",
        "    matmul_col<<<blocks, tpb>>>(\n",
        "        m.data_ptr<float>(), n.data_ptr<float>(), output.data_ptr<float>(), h, w, k);\n",
        "    C10_CUDA_KERNEL_LAUNCH_CHECK();\n",
        "    return output;\n",
        "}\n",
        "//!cuda\n",
        "\"\"\"\n",
        "\n",
        "cpp_src = \\\n",
        "\"\"\"\n",
        "//cuda\n",
        "torch::Tensor matmul(torch::Tensor m, torch::Tensor n);\n",
        "//!cuda\n",
        "\"\"\"\n",
        "\n",
        "module = torch.utils.cpp_extension.load_inline(\n",
        "    \"test_ext\", cpp_src, cuda_src,\n",
        "    functions=['matmul'], extra_cuda_cflags=['--ptxas-options=-v'], verbose=True)\n",
        "\n",
        "n = 32\n",
        "A = torch.randn(n, n, device='cuda')\n",
        "B = torch.randn(n, n, device='cuda')\n",
        "\n",
        "# A = torch.ones((3, 3), device='cuda')\n",
        "# B = torch.ones((3, 3), device='cuda')\n",
        "\n",
        "out = module.matmul(A, B); torch.cuda.synchronize()\n",
        "reference = torch.matmul(A, B)\n",
        "print(\"Out:\", out)\n",
        "print(\"Reference:\", reference)\n",
        "print(\"Correct Implementation:\", torch.allclose(out, reference))\n",
        "\n",
        "import time\n",
        "num_trials = 1_000\n",
        "\n",
        "with torch.profiler.profile() as prof:\n",
        "    for i in range(num_trials):\n",
        "        module.matmul(A, B)\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "print(prof.key_averages().table())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbIGnUTeYquH",
        "outputId": "79059f92-c3d2-4f2a-bde0-52bda9dadd01"
      },
      "id": "hbIGnUTeYquH",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
            "The input conditions for extension module test_ext have changed. Bumping to version 3 and re-building as test_ext_v3...\n",
            "Detected CUDA files, patching ldflags\n",
            "Emitting ninja build file /root/.cache/torch_extensions/py310_cu121/test_ext/build.ninja...\n",
            "Building extension module test_ext_v3...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "Loading extension module test_ext_v3...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Out: tensor([[-5.4679,  1.1751, -1.0962,  ...,  2.7903,  6.5714, -1.7081],\n",
            "        [ 6.8990, -6.2920, -4.9214,  ...,  2.4059, -9.0105,  3.9678],\n",
            "        [-4.2295,  1.0672, -5.3186,  ...,  1.6381,  3.7604, 11.1707],\n",
            "        ...,\n",
            "        [-0.5478,  3.4716, -0.5633,  ..., -0.1780,  2.3237, -5.7205],\n",
            "        [-0.3733, -7.9170, -1.0061,  ..., -3.9494,  0.3245, -1.2547],\n",
            "        [-4.4514,  1.6518, -1.3146,  ..., -3.8617, -3.5407,  4.0330]],\n",
            "       device='cuda:0')\n",
            "Reference: tensor([[-5.4679,  1.1751, -1.0962,  ...,  2.7903,  6.5714, -1.7081],\n",
            "        [ 6.8990, -6.2920, -4.9214,  ...,  2.4059, -9.0105,  3.9678],\n",
            "        [-4.2295,  1.0672, -5.3186,  ...,  1.6381,  3.7604, 11.1707],\n",
            "        ...,\n",
            "        [-0.5478,  3.4716, -0.5633,  ..., -0.1780,  2.3237, -5.7205],\n",
            "        [-0.3733, -7.9170, -1.0061,  ..., -3.9494,  0.3245, -1.2547],\n",
            "        [-4.4514,  1.6518, -1.3146,  ..., -3.8617, -3.5407,  4.0330]],\n",
            "       device='cuda:0')\n",
            "Correct Implementation: True\n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                            aten::zeros         9.05%       5.882ms        67.17%      43.636ms      43.636us       0.000us         0.00%       1.930ms       1.930us          1000  \n",
            "                                            aten::empty        20.82%      13.529ms        20.82%      13.529ms      13.529us       0.000us         0.00%       0.000us       0.000us          1000  \n",
            "                                            aten::zero_         7.65%       4.970ms        38.60%      25.075ms      25.075us       0.000us         0.00%       2.000ms       2.000us          1000  \n",
            "                                            aten::fill_        14.61%       9.493ms        30.95%      20.105ms      20.105us       2.000ms         3.85%       2.000ms       2.000us          1000  \n",
            "                                       cudaLaunchKernel        24.73%      16.069ms        24.73%      16.069ms       8.034us       0.000us         0.00%       0.000us       0.000us          2000  \n",
            "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       2.000ms         3.85%       2.000ms       2.000us          1000  \n",
            "      matmul_col(float*, float*, float*, int, int, int)         0.00%       0.000us         0.00%       0.000us       0.000us      50.000ms        96.15%      50.000ms      50.000us          1000  \n",
            "                                  cudaDeviceSynchronize        23.12%      15.023ms        23.12%      15.023ms      15.008us       0.000us         0.00%       0.000us       0.000us          1001  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 64.966ms\n",
            "Self CUDA time total: 52.000ms\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "c. Analyze the pros and cons of each of the two kernel designs.\n",
        "\n",
        "The pros & cons for the row wise and column wise matrix multiplication depends on the size of the matrices. Let A be of size (M x K), and B of size (K x N). If M > N, there are more rows than columns, so having the row-wise direction be paralelized is more beneficial so `matmul_row` is faster, and vice-versa."
      ],
      "metadata": {
        "id": "KvwAKv1ejYdN"
      },
      "id": "KvwAKv1ejYdN"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 2\n",
        "\n",
        "Write a matrix-vector multiplication kernel and the host stub function that can be called with four parameters: pointer to the output matrix, pointer to the input matrix, pointer to the input vector, and the number of elements in each dimension. Use one thread to calculate an output vector element."
      ],
      "metadata": {
        "id": "KzuPWug1pqJr"
      },
      "id": "KzuPWug1pqJr"
    },
    {
      "cell_type": "code",
      "source": [
        "cuda_src = cuda_begin + \\\n",
        "\"\"\"\n",
        "//cuda\n",
        "__global__ void matmul_col(float* m, float* n, float* out, int h, int w, int k) {\n",
        "    int c = blockIdx.x*blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (c >= w) return;\n",
        "\n",
        "    for (int r = 0; r < h; ++r) {\n",
        "        float o = 0;\n",
        "        for (int i = 0; i<k; ++i) {\n",
        "            o += m[r*k + i] * n[i*w + c];\n",
        "        }\n",
        "        out[r*w+c] = o;\n",
        "    }\n",
        "}\n",
        "\n",
        "torch::Tensor matmul(torch::Tensor m, torch::Tensor n) {\n",
        "    CHECK_INPUT(m); CHECK_INPUT(n);\n",
        "    int h = m.size(0);\n",
        "    int w = n.size(1);\n",
        "    int k = m.size(1);\n",
        "    TORCH_CHECK(k == n.size(0), \"Size mismatch!\");\n",
        "    auto output = torch::zeros({h, w}, m.options());\n",
        "\n",
        "    dim3 tpb(256);\n",
        "    dim3 blocks(cdiv(h, tpb.x));\n",
        "    matmul_col<<<blocks, tpb>>>(\n",
        "        m.data_ptr<float>(), n.data_ptr<float>(), output.data_ptr<float>(), h, w, k);\n",
        "    C10_CUDA_KERNEL_LAUNCH_CHECK();\n",
        "    return output;\n",
        "}\n",
        "//!cuda\n",
        "\"\"\"\n",
        "\n",
        "cpp_src = \\\n",
        "\"\"\"\n",
        "//cuda\n",
        "torch::Tensor matmul(torch::Tensor m, torch::Tensor n);\n",
        "//!cuda\n",
        "\"\"\"\n",
        "\n",
        "module = torch.utils.cpp_extension.load_inline(\n",
        "    \"test_ext\", cpp_src, cuda_src,\n",
        "    functions=['matmul'], extra_cuda_cflags=['--ptxas-options=-v'], verbose=True)\n",
        "\n",
        "n = 32\n",
        "A = torch.randn(n, n, device='cuda')\n",
        "B = torch.randn(n, n, device='cuda')\n",
        "\n",
        "# A = torch.ones((3, 3), device='cuda')\n",
        "# B = torch.ones((3, 3), device='cuda')\n",
        "\n",
        "out = module.matmul(A, B); torch.cuda.synchronize()\n",
        "reference = torch.matmul(A, B)\n",
        "print(\"Out:\", out)\n",
        "print(\"Reference:\", reference)\n",
        "print(\"Correct Implementation:\", torch.allclose(out, reference))\n",
        "\n",
        "import time\n",
        "num_trials = 1_000\n",
        "\n",
        "with torch.profiler.profile() as prof:\n",
        "    for i in range(num_trials):\n",
        "        module.matmul(A, B)\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "print(prof.key_averages().table())"
      ],
      "metadata": {
        "id": "jN9k9uAIhKtt"
      },
      "id": "jN9k9uAIhKtt",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.16"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}