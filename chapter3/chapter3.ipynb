{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ardywibowo/cuda-mode/blob/main/chapter3/chapter3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b34981f1-9278-4ee8-a2de-57c6245041a7",
      "metadata": {
        "id": "b34981f1-9278-4ee8-a2de-57c6245041a7"
      },
      "source": [
        "# Chapter 3"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21fce84e-7b13-446e-9504-eb037dc26d03",
      "metadata": {
        "id": "21fce84e-7b13-446e-9504-eb037dc26d03"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "92cc0e45",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92cc0e45",
        "outputId": "17fcae0e-4645-44e6-d5f0-69363d9eda97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ninja\n",
            "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/307.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m256.0/307.2 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ninja\n",
            "Successfully installed ninja-1.11.1.1\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [670 kB]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Get:7 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,408 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [1,704 kB]\n",
            "Hit:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [1,800 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,334 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,723 kB]\n",
            "Fetched 8,873 kB in 2s (3,782 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "33 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "g++-11 is already the newest version (11.4.0-1ubuntu1~22.04).\n",
            "g++-11 set to manually installed.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 33 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libhiredis0.14\n",
            "Suggested packages:\n",
            "  distcc | icecc\n",
            "The following NEW packages will be installed:\n",
            "  ccache libhiredis0.14\n",
            "0 upgraded, 2 newly installed, 0 to remove and 33 not upgraded.\n",
            "Need to get 528 kB of archives.\n",
            "After this operation, 1,469 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libhiredis0.14 amd64 0.14.1-2 [32.8 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 ccache amd64 4.5.1-1 [495 kB]\n",
            "Fetched 528 kB in 1s (453 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 2.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libhiredis0.14:amd64.\n",
            "(Reading database ... 121747 files and directories currently installed.)\n",
            "Preparing to unpack .../libhiredis0.14_0.14.1-2_amd64.deb ...\n",
            "Unpacking libhiredis0.14:amd64 (0.14.1-2) ...\n",
            "Selecting previously unselected package ccache.\n",
            "Preparing to unpack .../ccache_4.5.1-1_amd64.deb ...\n",
            "Unpacking ccache (4.5.1-1) ...\n",
            "Setting up libhiredis0.14:amd64 (0.14.1-2) ...\n",
            "Setting up ccache (4.5.1-1) ...\n",
            "Updating symlinks in /usr/lib/ccache ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ],
      "source": [
        "!pip install ninja\n",
        "!sudo apt update\n",
        "!sudo apt install g++-11 -y\n",
        "!sudo apt install ccache -y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "eWzrYElusXY6",
      "metadata": {
        "id": "eWzrYElusXY6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.utils.cpp_extension\n",
        "import os\n",
        "os.environ['CXX'] = '/usr/lib/ccache/g++-11'\n",
        "os.environ['CC'] = '/usr/lib/ccache/gcc-11'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "OaGV394NPYMl",
      "metadata": {
        "id": "OaGV394NPYMl"
      },
      "outputs": [],
      "source": [
        "cuda_begin = \"\"\"\n",
        "//cuda\n",
        "#include <torch/extension.h>\n",
        "#include <stdio.h>\n",
        "#include <c10/cuda/CUDAException.h>\n",
        "\n",
        "#define CHECK_CUDA(x) TORCH_CHECK(x.device().is_cuda(), #x \" must be a CUDA tensor\")\n",
        "#define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x \" must be contiguous\")\n",
        "#define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
        "\n",
        "inline unsigned int cdiv(unsigned int a, unsigned int b) { return (a + b - 1) / b;}\n",
        "//!cuda\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pW6QL__iY8Hm",
      "metadata": {
        "id": "pW6QL__iY8Hm"
      },
      "source": [
        "## Problem 1\n",
        "\n",
        "In this chapter we implemented a matrix multiplication kernel that has each thread produce one output matrix element. In this question, you will implement different matrix-matrix multiplication kernels and compare them.\n",
        "\n",
        "a. Write a kernel that has each thread produce one output matrix row. Fill in the execution configuration parameters for the design."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "w2RQMCWatB7r",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2RQMCWatB7r",
        "outputId": "c021aea7-7d81-45bc-b001-ce9894bf0524"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
            "No modifications detected for re-loaded extension module test_ext_v2, skipping build step...\n",
            "Loading extension module test_ext_v2...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Out: tensor([[ -2.4887,  -1.7471,   2.4387,  ...,   0.1727,   1.1549,   1.4592],\n",
            "        [ -3.4692,   8.6338,  -2.4712,  ...,   3.5144,   0.9575,   2.7203],\n",
            "        [ -6.9637,   5.5027,   1.5716,  ...,   3.4485,   0.2339,   6.3301],\n",
            "        ...,\n",
            "        [  1.2553,  -0.8796,  -5.3700,  ...,   1.4417,   1.7112,   3.1901],\n",
            "        [ 11.2950,   1.3072,  -3.6682,  ...,  10.2170,  -3.2171,  -1.7322],\n",
            "        [-10.9152,  -0.5431,  -4.4932,  ...,  -6.4946,  -2.2655,  -9.4765]],\n",
            "       device='cuda:0')\n",
            "Reference: tensor([[ -2.4887,  -1.7471,   2.4387,  ...,   0.1727,   1.1549,   1.4592],\n",
            "        [ -3.4692,   8.6338,  -2.4712,  ...,   3.5144,   0.9575,   2.7203],\n",
            "        [ -6.9637,   5.5027,   1.5716,  ...,   3.4485,   0.2339,   6.3301],\n",
            "        ...,\n",
            "        [  1.2553,  -0.8796,  -5.3700,  ...,   1.4417,   1.7112,   3.1901],\n",
            "        [ 11.2950,   1.3072,  -3.6682,  ...,  10.2170,  -3.2171,  -1.7322],\n",
            "        [-10.9152,  -0.5431,  -4.4932,  ...,  -6.4946,  -2.2655,  -9.4765]],\n",
            "       device='cuda:0')\n",
            "Correct Implementation: True\n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                            aten::zeros         4.08%       4.748ms        38.41%      44.696ms      44.696us       0.000us         0.00%       1.954ms       1.954us          1000  \n",
            "                                            aten::empty        10.03%      11.671ms        10.03%      11.671ms      11.671us       0.000us         0.00%       0.000us       0.000us          1000  \n",
            "                                            aten::zero_         3.93%       4.579ms        24.77%      28.827ms      28.827us       0.000us         0.00%       2.000ms       2.000us          1000  \n",
            "                                            aten::fill_         8.78%      10.215ms        20.84%      24.248ms      24.248us       2.000ms         2.38%       2.000ms       2.000us          1000  \n",
            "                                       cudaLaunchKernel        17.99%      20.936ms        17.99%      20.936ms      10.468us       0.000us         0.00%       0.000us       0.000us          2000  \n",
            "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       2.000ms         2.38%       2.000ms       2.000us          1000  \n",
            "      matmul_row(float*, float*, float*, int, int, int)         0.00%       0.000us         0.00%       0.000us       0.000us      82.000ms        97.62%      82.000ms      82.000us          1000  \n",
            "                                  cudaDeviceSynchronize        55.19%      64.219ms        55.19%      64.219ms      64.155us       0.000us         0.00%       0.000us       0.000us          1001  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 116.368ms\n",
            "Self CUDA time total: 84.000ms\n",
            "\n"
          ]
        }
      ],
      "source": [
        "cuda_src = cuda_begin + \\\n",
        "\"\"\"\n",
        "//cuda\n",
        "__global__ void matmul_row(float* m, float* n, float* out, int h, int w, int k) {\n",
        "    int r = blockIdx.x*blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (r >= h) return;\n",
        "\n",
        "    for (int c = 0; c < w; ++c) {\n",
        "        float o = 0;\n",
        "        for (int i = 0; i<k; ++i) {\n",
        "            o += m[r*k + i] * n[i*w + c];\n",
        "        }\n",
        "        out[r*w+c] = o;\n",
        "    }\n",
        "}\n",
        "\n",
        "torch::Tensor matmul(torch::Tensor m, torch::Tensor n) {\n",
        "    CHECK_INPUT(m); CHECK_INPUT(n);\n",
        "    int h = m.size(0);\n",
        "    int w = n.size(1);\n",
        "    int k = m.size(1);\n",
        "    TORCH_CHECK(k == n.size(0), \"Size mismatch!\");\n",
        "    auto output = torch::zeros({h, w}, m.options());\n",
        "\n",
        "    dim3 tpb(256);\n",
        "    dim3 blocks(cdiv(h, tpb.x));\n",
        "    matmul_row<<<blocks, tpb>>>(\n",
        "        m.data_ptr<float>(), n.data_ptr<float>(), output.data_ptr<float>(), h, w, k);\n",
        "    C10_CUDA_KERNEL_LAUNCH_CHECK();\n",
        "    return output;\n",
        "}\n",
        "//!cuda\n",
        "\"\"\"\n",
        "\n",
        "cpp_src = \\\n",
        "\"\"\"\n",
        "//cuda\n",
        "torch::Tensor matmul(torch::Tensor m, torch::Tensor n);\n",
        "//!cuda\n",
        "\"\"\"\n",
        "\n",
        "module = torch.utils.cpp_extension.load_inline(\n",
        "    \"test_ext\", cpp_src, cuda_src,\n",
        "    functions=['matmul'], extra_cuda_cflags=['--ptxas-options=-v'], verbose=True)\n",
        "\n",
        "n = 32\n",
        "A = torch.randn(n, n, device='cuda')\n",
        "B = torch.randn(n, n, device='cuda')\n",
        "\n",
        "A = torch.ones((3, 3), device='cuda')\n",
        "B = torch.ones((3, 3), device='cuda')\n",
        "\n",
        "out = module.matmul(A, B); torch.cuda.synchronize()\n",
        "reference = torch.matmul(A, B)\n",
        "print(\"Out:\", out)\n",
        "print(\"Reference:\", reference)\n",
        "print(\"Correct Implementation:\", torch.allclose(out, reference))\n",
        "\n",
        "import time\n",
        "num_trials = 1_000\n",
        "\n",
        "with torch.profiler.profile() as prof:\n",
        "    for i in range(num_trials):\n",
        "        module.matmul(A, B)\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "print(prof.key_averages().table())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oCjfrcr1hzdT",
      "metadata": {
        "id": "oCjfrcr1hzdT"
      },
      "source": [
        "b. Write a kernel that has each thread produce one output matrix column. Fill in the execution configuration parameters for the design."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "hbIGnUTeYquH",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbIGnUTeYquH",
        "outputId": "dd4ccbb5-4f96-4695-ad65-9674851f0fe8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
            "The input conditions for extension module test_ext have changed. Bumping to version 1 and re-building as test_ext_v1...\n",
            "Detected CUDA files, patching ldflags\n",
            "Emitting ninja build file /root/.cache/torch_extensions/py310_cu121/test_ext/build.ninja...\n",
            "Building extension module test_ext_v1...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "Loading extension module test_ext_v1...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Out: tensor([[ -0.0979,   6.7681,   0.6130,  ...,  -1.8185,   3.3314,  -2.8923],\n",
            "        [  6.0187,   0.3002,  10.5269,  ...,   5.2424,  -6.9971,  -7.3472],\n",
            "        [ -4.5366,   3.3448,  -6.6962,  ...,  -5.8891,   1.3179, -12.7183],\n",
            "        ...,\n",
            "        [ -1.6748,   1.6266, -16.4629,  ...,  -8.2513,  -6.5285,  -0.9741],\n",
            "        [ -2.6126,  -2.9857,   8.3969,  ...,   7.6670,   7.5929,  -0.3515],\n",
            "        [ 11.0854,  -2.4087,  -1.2288,  ...,  -7.0006,   5.3422,   5.4572]],\n",
            "       device='cuda:0')\n",
            "Reference: tensor([[ -0.0979,   6.7681,   0.6130,  ...,  -1.8185,   3.3314,  -2.8923],\n",
            "        [  6.0187,   0.3002,  10.5269,  ...,   5.2424,  -6.9971,  -7.3472],\n",
            "        [ -4.5366,   3.3448,  -6.6962,  ...,  -5.8891,   1.3179, -12.7183],\n",
            "        ...,\n",
            "        [ -1.6748,   1.6266, -16.4629,  ...,  -8.2513,  -6.5285,  -0.9741],\n",
            "        [ -2.6126,  -2.9857,   8.3969,  ...,   7.6670,   7.5929,  -0.3515],\n",
            "        [ 11.0854,  -2.4087,  -1.2288,  ...,  -7.0006,   5.3422,   5.4572]],\n",
            "       device='cuda:0')\n",
            "Correct Implementation: True\n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                            aten::zeros        10.52%       8.088ms        47.45%      36.466ms      36.466us       0.000us         0.00%       1.884ms       1.884us          1000  \n",
            "                                            aten::empty        15.11%      11.616ms        15.11%      11.616ms      11.616us       0.000us         0.00%       0.000us       0.000us          1000  \n",
            "                                            aten::zero_         4.10%       3.148ms        23.03%      17.696ms      17.696us       0.000us         0.00%       2.000ms       2.000us          1000  \n",
            "                                            aten::fill_         8.46%       6.504ms        18.93%      14.548ms      14.548us       2.000ms         3.85%       2.000ms       2.000us          1000  \n",
            "                                       cudaLaunchKernel        16.37%      12.579ms        16.37%      12.579ms       6.290us       0.000us         0.00%       0.000us       0.000us          2000  \n",
            "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       2.000ms         3.85%       2.000ms       2.000us          1000  \n",
            "      matmul_col(float*, float*, float*, int, int, int)         0.00%       0.000us         0.00%       0.000us       0.000us      50.005ms        96.15%      50.005ms      50.005us          1000  \n",
            "                                  cudaDeviceSynchronize        45.43%      34.916ms        45.43%      34.916ms      34.881us       0.000us         0.00%       0.000us       0.000us          1001  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 76.851ms\n",
            "Self CUDA time total: 52.005ms\n",
            "\n"
          ]
        }
      ],
      "source": [
        "cuda_src = cuda_begin + \\\n",
        "\"\"\"\n",
        "//cuda\n",
        "__global__ void matmul_col(float* m, float* n, float* out, int h, int w, int k) {\n",
        "    int c = blockIdx.x*blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (c >= w) return;\n",
        "\n",
        "    for (int r = 0; r < h; ++r) {\n",
        "        float o = 0;\n",
        "        for (int i = 0; i<k; ++i) {\n",
        "            o += m[r*k + i] * n[i*w + c];\n",
        "        }\n",
        "        out[r*w+c] = o;\n",
        "    }\n",
        "}\n",
        "\n",
        "torch::Tensor matmul(torch::Tensor m, torch::Tensor n) {\n",
        "    CHECK_INPUT(m); CHECK_INPUT(n);\n",
        "    int h = m.size(0);\n",
        "    int w = n.size(1);\n",
        "    int k = m.size(1);\n",
        "    TORCH_CHECK(k == n.size(0), \"Size mismatch!\");\n",
        "    auto output = torch::zeros({h, w}, m.options());\n",
        "\n",
        "    dim3 tpb(256);\n",
        "    dim3 blocks(cdiv(h, tpb.x));\n",
        "    matmul_col<<<blocks, tpb>>>(\n",
        "        m.data_ptr<float>(), n.data_ptr<float>(), output.data_ptr<float>(), h, w, k);\n",
        "    C10_CUDA_KERNEL_LAUNCH_CHECK();\n",
        "    return output;\n",
        "}\n",
        "//!cuda\n",
        "\"\"\"\n",
        "\n",
        "cpp_src = \\\n",
        "\"\"\"\n",
        "//cuda\n",
        "torch::Tensor matmul(torch::Tensor m, torch::Tensor n);\n",
        "//!cuda\n",
        "\"\"\"\n",
        "\n",
        "module = torch.utils.cpp_extension.load_inline(\n",
        "    \"test_ext\", cpp_src, cuda_src,\n",
        "    functions=['matmul'], extra_cuda_cflags=['--ptxas-options=-v'], verbose=True)\n",
        "\n",
        "n = 32\n",
        "A = torch.randn(n, n, device='cuda')\n",
        "B = torch.randn(n, n, device='cuda')\n",
        "\n",
        "# A = torch.ones((3, 3), device='cuda')\n",
        "# B = torch.ones((3, 3), device='cuda')\n",
        "\n",
        "out = module.matmul(A, B); torch.cuda.synchronize()\n",
        "reference = torch.matmul(A, B)\n",
        "print(\"Out:\", out)\n",
        "print(\"Reference:\", reference)\n",
        "print(\"Correct Implementation:\", torch.allclose(out, reference))\n",
        "\n",
        "import time\n",
        "num_trials = 1_000\n",
        "\n",
        "with torch.profiler.profile() as prof:\n",
        "    for i in range(num_trials):\n",
        "        module.matmul(A, B)\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "print(prof.key_averages().table())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KvwAKv1ejYdN",
      "metadata": {
        "id": "KvwAKv1ejYdN"
      },
      "source": [
        "c. Analyze the pros and cons of each of the two kernel designs.\n",
        "\n",
        "The pros & cons for the row wise and column wise matrix multiplication depends on the size of the matrices. Let A be of size (M x K), and B of size (K x N). If M > N, there are more rows than columns, so having the row-wise direction be paralelized is more beneficial so `matmul_row` is faster, and vice-versa."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KzuPWug1pqJr",
      "metadata": {
        "id": "KzuPWug1pqJr"
      },
      "source": [
        "## Problem 2\n",
        "\n",
        "Write a matrix-vector multiplication kernel and the host stub function that can be called with four parameters: pointer to the output matrix, pointer to the input matrix, pointer to the input vector, and the number of elements in each dimension. Use one thread to calculate an output vector element."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "jN9k9uAIhKtt",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jN9k9uAIhKtt",
        "outputId": "8fc290e1-6883-40a3-9115-5795ac4ee447"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using /root/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
            "The input conditions for extension module test_ext have changed. Bumping to version 2 and re-building as test_ext_v2...\n",
            "Detected CUDA files, patching ldflags\n",
            "Emitting ninja build file /root/.cache/torch_extensions/py310_cu121/test_ext/build.ninja...\n",
            "Building extension module test_ext_v2...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "Loading extension module test_ext_v2...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Out: tensor([[  2.1938,   7.5036,  -7.4128,  ...,   0.8692,   0.5169,   0.6550],\n",
            "        [  1.0481,   7.5440,   4.3394,  ...,   4.4534,   2.5028,  -4.3398],\n",
            "        [  6.1509,  -8.3925,   8.1037,  ..., -10.0252,   0.4850, -12.5438],\n",
            "        ...,\n",
            "        [  0.2771,  -4.8176, -11.1387,  ...,   3.7976,  -7.6448,   6.6561],\n",
            "        [-11.1273,  14.3485,   2.7231,  ...,   0.8672,  -0.7542,  -3.8515],\n",
            "        [  5.3164,  -0.9553,  10.9804,  ...,  -1.7326,   4.1123,   1.0089]],\n",
            "       device='cuda:0')\n",
            "Reference: tensor([[  2.1938,   7.5036,  -7.4128,  ...,   0.8692,   0.5169,   0.6550],\n",
            "        [  1.0481,   7.5440,   4.3394,  ...,   4.4534,   2.5028,  -4.3398],\n",
            "        [  6.1509,  -8.3925,   8.1037,  ..., -10.0252,   0.4850, -12.5438],\n",
            "        ...,\n",
            "        [  0.2771,  -4.8176, -11.1387,  ...,   3.7976,  -7.6448,   6.6561],\n",
            "        [-11.1273,  14.3485,   2.7231,  ...,   0.8672,  -0.7542,  -3.8515],\n",
            "        [  5.3164,  -0.9553,  10.9804,  ...,  -1.7326,   4.1123,   1.0089]],\n",
            "       device='cuda:0')\n",
            "Correct Implementation: True\n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                            aten::zeros         9.25%       5.119ms        79.32%      43.887ms      43.887us       0.000us         0.00%       1.928ms       1.928us          1000  \n",
            "                                            aten::empty        24.53%      13.571ms        24.53%      13.571ms      13.571us       0.000us         0.00%       0.000us       0.000us          1000  \n",
            "                                            aten::zero_         7.62%       4.214ms        47.25%      26.142ms      26.142us       0.000us         0.00%       2.000ms       2.000us          1000  \n",
            "                                            aten::fill_        17.75%       9.821ms        39.63%      21.928ms      21.928us       2.000ms        16.64%       2.000ms       2.000us          1000  \n",
            "                                       cudaLaunchKernel        31.49%      17.422ms        31.49%      17.422ms       8.711us       0.000us         0.00%       0.000us       0.000us          2000  \n",
            "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       2.000ms        16.64%       2.000ms       2.000us          1000  \n",
            "matmul_kernel(float*, float*, float*, int, int, int)...         0.00%       0.000us         0.00%       0.000us       0.000us      10.016ms        83.36%      10.016ms      10.016us          1000  \n",
            "                                  cudaDeviceSynchronize         9.36%       5.179ms         9.36%       5.179ms       5.174us       0.000us         0.00%       0.000us       0.000us          1001  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 55.326ms\n",
            "Self CUDA time total: 12.016ms\n",
            "\n"
          ]
        }
      ],
      "source": [
        "cuda_src = cuda_begin + \\\n",
        "\"\"\"\n",
        "//cuda\n",
        "__global__ void matmul_kernel(float* m, float* n, float* out, int h, int w, int k) {\n",
        "    int c = blockIdx.x*blockDim.x + threadIdx.x;\n",
        "    int r = blockIdx.y*blockDim.y + threadIdx.y;\n",
        "\n",
        "    if (c >= w || r >= h) return;\n",
        "\n",
        "    float o = 0;\n",
        "    for (int i = 0; i<k; ++i) {\n",
        "        o += m[r*k + i] * n[i*w + c];\n",
        "    }\n",
        "    out[r*w+c] = o;\n",
        "}\n",
        "\n",
        "torch::Tensor matmul(torch::Tensor m, torch::Tensor n) {\n",
        "    CHECK_INPUT(m); CHECK_INPUT(n);\n",
        "    int h = m.size(0);\n",
        "    int w = n.size(1);\n",
        "    int k = m.size(1);\n",
        "    TORCH_CHECK(k == n.size(0), \"Size mismatch!\");\n",
        "    auto output = torch::zeros({h, w}, m.options());\n",
        "\n",
        "    dim3 tpb(32, 32);\n",
        "    dim3 blocks(cdiv(w, tpb.x), cdiv(h, tpb.y));\n",
        "    matmul_kernel<<<blocks, tpb>>>(\n",
        "        m.data_ptr<float>(), n.data_ptr<float>(), output.data_ptr<float>(), h, w, k);\n",
        "    C10_CUDA_KERNEL_LAUNCH_CHECK();\n",
        "    return output;\n",
        "}\n",
        "//!cuda\n",
        "\"\"\"\n",
        "\n",
        "cpp_src = \\\n",
        "\"\"\"\n",
        "//cuda\n",
        "torch::Tensor matmul(torch::Tensor m, torch::Tensor n);\n",
        "//!cuda\n",
        "\"\"\"\n",
        "\n",
        "module = torch.utils.cpp_extension.load_inline(\n",
        "    \"test_ext\", cpp_src, cuda_src,\n",
        "    functions=['matmul'], extra_cuda_cflags=['--ptxas-options=-v'], verbose=True)\n",
        "\n",
        "n = 32\n",
        "A = torch.randn(n, n, device='cuda')\n",
        "B = torch.randn(n, n, device='cuda')\n",
        "\n",
        "# A = torch.ones((3, 3), device='cuda')\n",
        "# B = torch.ones((3, 3), device='cuda')\n",
        "\n",
        "out = module.matmul(A, B); torch.cuda.synchronize()\n",
        "reference = torch.matmul(A, B)\n",
        "print(\"Out:\", out)\n",
        "print(\"Reference:\", reference)\n",
        "print(\"Correct Implementation:\", torch.allclose(out, reference))\n",
        "\n",
        "import time\n",
        "num_trials = 1_000\n",
        "\n",
        "with torch.profiler.profile() as prof:\n",
        "    for i in range(num_trials):\n",
        "        module.matmul(A, B)\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "print(prof.key_averages().table())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "322546eb",
      "metadata": {},
      "source": [
        "## Problem 3\n",
        "\n",
        "Consider the following CUDA kernel and the corresponding host function that calls it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "affa698b",
      "metadata": {},
      "outputs": [],
      "source": [
        "cuda_kernel = '''\n",
        "//cuda\n",
        "__global__ void foo_kernel(float* a, float* b, unsigned int M, unsigned int N) {\n",
        "    unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    unsigned int j = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    if (i < M && j < N) {\n",
        "        b[i * N + j] = a[i * N + j] / 2.1f + 4.8f;\n",
        "    }\n",
        "}\n",
        "\n",
        "void foo(float* a_d, gloat* b_d) {\n",
        "    unsigned int M = 150;\n",
        "    unsigned int N = 300;\n",
        "    dim3 bd(32, 32);\n",
        "    dim3 gd((N-1)/16 + 1, (M-1)/32 + 1);\n",
        "    foo_kernel<<<gd, bd>>>(a_d, b_d, M, N);\n",
        "}\n",
        "//!cuda\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "080611dc",
      "metadata": {},
      "source": [
        "What is the number of threads per block?\n",
        "- There are `(floor((300-1)/16) + 1) * (floor((150-1)/32) + 1) = 95` threads per block\n",
        "\n",
        "What is the number of threads in the grid?\n",
        "- There are `95 * 32 * 32 = 97280` threads in the grid\n",
        "\n",
        "What is the number of blocks in the grid?\n",
        "- There are `32 * 32 = 1024` blocks in the grid.\n",
        "\n",
        "What is the number of threads that execute the code on line 05?\n",
        "- There are `150 * 300 = 45000` threads that execute the code on line 05."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49a44d0e",
      "metadata": {},
      "source": [
        "Consider a 2D matrix with a width of 400 and a height of 500. The matrix is stored as a one-dimensional array. Specify the array index of the matrix element at row 20 and column 10:\n",
        "\n",
        "If the matrix is stored in row-major order.\n",
        "- The array index is `20 * 400 + 10 = 8010`\n",
        "\n",
        "If the matrix is stored in column-major order.\n",
        "- The array index is `10 * 500 + 20 = 5020`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "699c42a5",
      "metadata": {},
      "source": [
        "Consider a 3D tensor with a width of 400, a height of 500, and a depth of 300. The tensor is stored as a one-dimensional array in row-major order. Specify the array index of the tensor element at x = 10, y = 20, and z = 5.\n",
        "- The array index is `5 * 400 * 500 + 20 * 400 + 10 = 10002010`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb7d74ad",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
